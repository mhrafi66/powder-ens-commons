<!-- public/unet_localization.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>UNet-BSpline Localization Model</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #f9f9f9;
      color: #2c3e50;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.7;
    }
    h1, h2, h3 {
      color: #007acc;
    }
    pre {
      background: #f4f4f4;
      padding: 10px;
      overflow-x: auto;
      border-left: 4px solid #007acc;
    }
    code {
      background: #f1f1f1;
      padding: 2px 4px;
      font-family: Consolas, monospace;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 10px;
      text-align: left;
    }
    th {
      background-color: #eef3f7;
    }
    a {
      color: #007acc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    hr {
      margin: 40px 0;
    }
  </style>
</head>
<body>

<h1>üì° Localization UNet with B-Spline Augmentation</h1>
<p>A physics-aware UNet-based localization model trained with B-spline augmented synthetic data, suitable for wireless signal-based transmitter localization tasks.</p>

<hr>

<h2>üß† UNet-BSpline Localization Model Commons</h2>
<p><strong>Repository:</strong> <a href="https://pypi.org/project/powder-ens-commons/">powder_ens_commons</a><br>
<strong>Model:</strong> <code>localization_unet_bspline</code><br>
<strong>Task:</strong> Wireless Localization from RSS Images<br>
<strong>Framework:</strong> PyTorch</p>

<hr>

<h2>üöÄ Overview</h2>
<p>This model implements a <strong>UNet-based deep learning model for transmitter localization</strong>, enhanced by a <strong>B-spline-based calibration</strong>. It is designed for wireless propagation modeling tasks using RSS maps generated from field-deployed sensor data.</p>

<p>The module is part of the <a href="https://pypi.org/project/powder-ens-commons/">powder_ens_commons</a> package, built to support wireless data and model sharing across experiments on the <a href="https://powderwireless.net/">POWDER platform</a>.</p>

<hr>

<h2>üîß How to Use</h2>

<h3>Installation</h3>
<pre><code>pip install powder_ens_commons</code></pre>

<h3>Example: Loading the Model in Google Colab</h3>
<pre><code>from powder_ens_commons.modelcommons.localization_unet_bspline import load_model_and_fit_dataset

# Load JSON dataset (ensure structure below)
import json
with open('my_dataset.json', 'r') as f:
    dataset = json.load(f)

# Load Model and Fine Tune
model, train_loss_arr, test_errors = load_model_and_fit_dataset(
    train_data=dataset,
    receivers_list=receivers,
    bmap=building_map,
    dsm=dsm_map,
    one_tx=False,
    device='cuda'  # Use 'cpu' if running without GPU
)</code></pre>

<hr>

<h2>üìÅ Dataset Format (Required for Current Version)</h2>
<p>Currently, the model only supports datasets in <code>.json</code> format with the following structure:</p>
<pre><code>"2022-04-25 14:11:02": {
  "rx_data": [
    [RSS, lat, lon, sensor_id],
    ...
  ],
  "tx_coords": [
    [lat, lon],
    ...
  ],
  "metadata": [
    {"power": ..., "transport": ..., "radio": ...},
    ...
  ]
}</code></pre>

<p>Example <code>rx_data</code> entry:</p>
<pre><code>[
  -75.14, 40.76, -111.85, "bus-4603"
]</code></pre>

<p>üìå <strong>Note:</strong> Future releases will support <code>.csv</code> and other formats.</p>

<hr>

<h2>üß™ Output</h2>
<p>The <code>load_model_and_fit_dataset</code> function returns:</p>
<ul>
  <li>The trained model</li>
  <li>Training loss array</li>
  <li>Final test error dictionary</li>
</ul>

<hr>

<h2>‚ö†Ô∏è Model Architecture</h2>

<p>
  The <code>localization_unet_bspline</code> model is based on a U-Net-style encoder-decoder architecture with skip connections.
  It is specifically designed for the wireless localization task using image-to-image regression.
  This model accepts a <strong>3-channel input image</strong>, each channel encoding a distinct signal or environment-related modality:
</p>

<h3>üß† Input Representation (3-Channel Image)</h3>

<ol>
  <li>
    <strong>Channel 1: RSS Image</strong>
    <ul>
      <li>A grayscale image where each receiver's location is illuminated based on the received signal strength (RSS).</li>
      <li>The intensity of the pixel represents the raw RSS value received at that location.</li>
    </ul>
  </li>

  <li>
    <strong>Channel 2: B-Spline Calibrated RSS Image</strong>
    <ul>
      <li>The raw RSS values are passed through a shallow B-spline neural network that learns to calibrate signal distortions due to hardware/environmental variability.</li>
      <li>A new grayscale image is generated from these calibrated values to form the second input channel.</li>
    </ul>
  </li>

  <li>
    <strong>Channel 3: Terrain-Aware Feature Map</strong>
    <ul>
      <li>This channel encodes geographical or environmental features from a satellite or terrain map.</li>
      <li>A pretrained or manually crafted encoder extracts a tensor representation corresponding to each location.</li>
    </ul>
  </li>
</ol>

<p>
  These three modalities are stacked together to form a 3-channel input tensor of shape:
  <code>(3, H, W)</code>.
</p>

<hr>
<h3>üß± U-Net Backbone</h3>
<p>The model follows a deep <strong>U-Net</strong> architecture, consisting of:</p>

<ul>
  <li>
    <strong>Downsampling path (Encoder):</strong>
    <ul>
      <li>Applies multiple stages of <code>3√ó3</code> convolution + ReLU layers followed by <code>2√ó2</code> max-pooling to reduce spatial dimensions.</li>
      <li>At each downsampling level, the number of feature channels doubles (e.g., <code>n ‚Üí 2n ‚Üí 4n ‚Üí 8n ‚Üí 16n</code>).</li>
    </ul>
  </li>

  <li>
    <strong>Upsampling path (Decoder):</strong>
    <ul>
      <li>Uses <code>2√ó2</code> up-convolution layers to restore spatial resolution, while concatenating corresponding encoder features via <strong>skip connections</strong>.</li>
      <li>Channel sizes decrease accordingly (<code>16n ‚Üí 8n ‚Üí 4n ‚Üí 2n ‚Üí n</code>).</li>
    </ul>
  </li>

  <li>
    <strong>Residual Skip Connections:</strong>
    <ul>
      <li>High-resolution information from earlier encoder layers is passed directly to decoder layers to help refine localization outputs, particularly in preserving fine-grained RSS signals.</li>
    </ul>
  </li>
</ul>

<hr>

<h3>üéØ Output</h3>
<ul>
  <li>The final output is a <strong>single-channel image</strong> <code>(1, H, W)</code> that encodes a <strong>heatmap</strong> over the region of interest.</li>
  <li>This heatmap represents the predicted transmitter location(s) using <strong>image-to-image regression</strong>.</li>
</ul>

<hr>

<h3>üß™ Optimization Objective</h3>
<ul>
  <li>
    The model uses an <strong>approximate-EMD (Earth Mover‚Äôs Distance)</strong> loss, which is particularly effective for spatial prediction tasks.
  </li>
  <li>
    This loss penalizes pixel-wise deviations proportionally to their spatial displacement, allowing smoother and more realistic localization.
  </li>
</ul>

<hr>

<h3>üîç Observations</h3>
<ul>
  <li>
    Although both direct coordinate prediction and image-based regression were explored,
    <strong>image-to-image prediction yielded the most accurate results</strong>.
  </li>
  <li>
    Despite penalizing small and large spatial errors equally, the model learned robust localization patterns through dense gradient flow and terrain-aware representations.
  </li>
</ul>

<hr>

<h3>üìä Summary</h3>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Input Channels</td>
      <td>RSS image, Calibrated RSS via B-spline, Terrain tensor</td>
    </tr>
    <tr>
      <td>Encoder-Decoder</td>
      <td>U-Net with skip connections (downsample + upsample stages)</td>
    </tr>
    <tr>
      <td>Output</td>
      <td>Heatmap of likely transmitter locations</td>
    </tr>
    <tr>
      <td>Loss</td>
      <td>Approximate EMD (Wasserstein-1 surrogate)</td>
    </tr>
    <tr>
      <td>Input Format</td>
      <td>Image: <code>(3, H, W)</code> ‚Üí Output: <code>(1, H, W)</code></td>
    </tr>
  </tbody>
</table>

<hr>

<h2>üìö Citation</h2>
<p>
Mitchell, F., Baset, A., Patwari, N., Kasera, S. K., & Bhaskara, A. (2022).  
<i>Deep Learning-based Localization in Limited Data Regimes.</i>  
In Proceedings of WiseML '22.  
<a href="https://doi.org/10.1145/3522783.3529529">https://doi.org/10.1145/3522783.3529529</a>
</p>

<hr>

<h2>üè∑Ô∏è License</h2>
<p>MIT License (or your preferred open-source license)</p>

<p><a href="index.html">‚Üê Back to main page</a></p>

</body>
</html>
